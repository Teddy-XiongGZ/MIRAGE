<style>
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%; /* or any desired width */
  }
</style>

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A benchmark for Retrieval-Augemented Generation on medical question answering.">
  <meta name="keywords" content="RAG, medicine, question answering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Benchmarking Retrieval-Augmented Generation for Medicine</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Benchmarking Retrieval-Augmented Generation for Medicine</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_rp4-a4AAAAJ&hl">Guangzhi Xiong</a><sup>♣†</sup>,</span>
            <span class="author-block">
              <a href="https://andy-jqa.github.io/">Qiao Jin</a><sup>♡†</sup>,</span>
            <span class="author-block">
              <a href="https://www.ncbi.nlm.nih.gov/research/bionlp/Zhiyong-Lu">Zhiyong Lu</a><sup>♡§</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.virginia.edu/~az9eg/website/home.html">Aidong Zhang</a><sup>♣§</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>♣</sup>University of Virginia,</span>
            <span class="author-block"><sup>♡</sup>National Library of Medicine, National Institutes of Health</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">†Equal Contribution,</span>
            <span class="author-block">§Co-correspondence</span><br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Teddy-XiongGZ/MIRAGE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/MedRAG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. 
            Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. 
          However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. 
          </p>
          
          <p>
            To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (<span class="dnerf">Mirage</span>), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using <span class="dnerf">Mirage</span>, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the <span class="dnerf">MedRag</span> toolkit introduced in this work. 
          </p>
          
          <p>
            Overall, <span class="dnerf">MedRag</span> improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the ``lost-in-the-middle'' effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mmmu">Overview</h1>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- MIRAGE. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3"><span class="dnerf">Mirage</span></h2>
          <p>
            <span class="dnerf">Mirage</span> is our proposed benchmark for Medical Information Retrieval-Augmented Generation Evaluation, which includes 7,663 questions from five commonly used
            QA datasets in biomedicine.
            It adopts four key evalution setting: 1) zero-shot learning, 2) multi-choice evaluation, 3) retrieval-augmented generation, 4) question-only retrieval. 
          </p>
          <img src="./figs/MIRAGE.png" alt="" class="center" width="100%"/>
        </div>
      </div>
      <!--/ MIRAGE. -->

      <!-- MedRAG. -->
      <div class="column">
        <h2 class="title is-3"><span class="dnerf">MedRag</span></h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              <span class="dnerf">MedRag</span> a systematic toolkit for Retrieval-Augmented Generation (RAG) on medical question answering (QA), which covers five corpora, four retrievers, and six LLMs including both general and domain-specific models.
              For all LLMs, it concatenates and prepends retrieved snippets from corpora to the question input, and perform chain-of-thought (CoT) prompting to fully leverage the reasoning capability of the models. 
            </p>
            <img src="./figs/MedRAG.png" alt="" class="center" width="100%"/>
          </div>

        </div>
      </div>
    </div>
    <!--/ MedRAG. -->

  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mmmu">Main Results</h1>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- LLM. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison of LLMs</h2>
        <div class="content has-text-justified">
          <p>
            While the best average score of other backbone LLMs can only achieve about 61% (GPT-3.5 and Mixtral) in the CoT setting, their performance can be significantly improved to around 70% with <span class="dnerf">MedRag</span>, which is comparable to GPT-4 (CoT). These results suggest the great potential of RAG as a way to enhance the zero-shot capability of LLMs to answer medical questions, which can be a more efficient choice than performing larger-scale pre-training. Our results also demonstrate that domain-specific LLMs can exhibit advantages in certain
            cases
          </p>
          <img src="./figs/result_llm.png" alt="" class="center" width="80%"/>
          <p class="has-text-centered">
           Table: Benchmark results of different backbone LLMs on <span class="dnerf">Mirage</span>. All numbers are accuracy in percentages.
          </p>
        </div>
      </div>
    </div>
    <!--/ LLM. -->
</div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Corpora and Retrievers. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison of Corpora and Retrievers</h2>
        <div class="content has-text-justified">
          <p>
          As shown in the table below, the performance of one RAG system is strongly related to the corpus it selects. It shows the variable performance of different retrievers, which can be
          explained by the data and strategy differences in their training. The fusion of retrieval results with RRF can effectively improves the performance, but may not always lead to a better performance.
          </p>
          <img src="./figs/result_corpus_retriever.png" alt="" class="center" width="80%"/>
          <p class="has-text-centered">
            Table: Accuracy (%) of GPT-3.5 (<span class="dnerf">MedRag</span>) with different corpora and retrievers on <span class="dnerf">Mirage</span>. Red and green
            denote performance decreases and increases compared to CoT (first row). The shade reflects the relative change. 
           </p>
        </div>
      </div>
    </div>
    <!--/ Corpora and Retrievers. -->
</div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mmmu">Further Discussion</h1>
  </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Scaling. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance Scaling</h2>
        <div class="content has-text-justified">
          <p>
            The figure below shows the scaling curves of <span class="dnerf">MedRag</span> on each task in <span class="dnerf">Mirage</span> with different numbers of snippets k ∈ {1, 2, 4, ..., 64}. On MMLU-Med, MedQA-US, and MedMCQA, we see roughly log-linear curves in the scaling plots for k ≤ 32.
            Compared with the three examination tasks, PubMedQA* and BioASQ-Y/N can be relatively easier for <span class="dnerf">MedRag</span> since the ground-truth supporting information can be found in PubMed.
          </p>
          <img src="./figs/discussion_scaling.png" alt="" class="center" width="100%"/>
          <p class="has-text-centered">
            Figure: <span class="dnerf">MedRag</span> accuracy with different numbers of retrieved snippets. Red dotted lines denote CoT performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Scaling. -->
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Position. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Position of Ground-truth Snippet</h2>
          <p>
            This following figure shows the changes in model accuracy
            corresponding to different parts of context locations. From the figure, we can see a clear U-shaped
            decreasing-then-increasing pattern in the accuracy
            change concerning the position of ground-truth
            snippets.
          </p>
          <img src="./figs/discussion_position.png" alt="" class="center" width="100%"/>
          <p class="has-text-centered">
            Figure: The relations between QA accuracy and the position of the ground-truth snippet in the LLM context.
          </p>
        </div>
      </div>
      <!--/ Position. -->

      <!-- Proportion. -->
      <div class="column">
        <h2 class="title is-3">Proportion in the MedCorp Corpus</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              This figure displays the proportions of
              four different sources in MedCorp and the actually retrieved sources in the top 64 retrieved snippets for each task in <span class="dnerf">Mirage</span>, which shows task-specific patterns.
              <!-- In general, the proportion of Wikipedia drops in the retrieved snippets for medical questions, which is expected as many snippets
              in Wikipedia are not related to biomedicine. -->
            </p>
            <img src="./figs/discussion_proportion.png" alt="" class="center" width="100%"/>
          <p class="has-text-centered">
            Figure: The overall corpus composition of MedCorp and the actually retrieved proportion in different tasks.
          </p>
        </div>
        </div>
      </div>
      <!--/ Proportion. -->      
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mmmu">Practical Recommendations</h1>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Corpus selection</h2>
        <div class="content has-text-justified">
          <p>
            Results above indicate that PubMed and the MedCorp corpus are the only corpora with which <span class="dnerf">MedRag</span> can outperform CoT on all tasks in <span class="dnerf">Mirage</span>. As a large-scale corpus, PubMed serves as a suitable document collection for various kinds of medical questions. If resources permit, the MedCorp corpus could be a more comprehensive and reliable choice: Nearly all <span class="dnerf">MedRag</span> settings using the MedCorp Corpus show improved performance (green-coded cells) compared to the CoT prompting baseline. In general, single corpora other than PubMed are not recommended for medical QA due to their limited volumes of medical knowledge, but they can also be beneficial in specific tasks such as question answering for medical examinations.
          </p>
        </div>
      </div>
    </div>
</div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Retriever selection</h2>
        <div class="content has-text-justified">
          <p>
            Among the four individual retrievers used in <span class="dnerf">MedRag</span>, MedCPT is the most reliable one which constantly outperforms other candidates with a higher average score on <span class="dnerf">Mirage</span>. BM25 is a strong retriever as well, which is also supported by other evaluations. The fusion of retrievers can provide robust performance but must be utilized with caution for the retrievers included. As for the PubMed corpus recommended above, a RRF-2 retriever that combines the results from BM25 and MedCPT can be a good selection, since they perform better than the other two with snippets from PubMed. For the MedCorp corpus, both RRF-2 and RRF-4 can be reliable choices, as the corpus can benefit all four individual retrievers in <span class="dnerf">MedRag</span>. 
          </p>
        </div>
      </div>
    </div>
</div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LLM selection</h2>
        <div class="content has-text-justified">
          <p>
            Currently, GPT-4 is the best model with about 80% accuracy on <span class="dnerf">Mirage</span>. However, it is much more expensive than other backbone LLMs. GPT-3.5 can be a more cost-efficient choice than GPT-4, which shows great capabilities of following <span class="dnerf">MedRag</span> instructions. For high-stakes scenarios such as medical diagnoses where patient privacy should be a key concern, the best open-source Mixtral model, which can be deployed locally and run offline, could be a viable option.
          </p>
        </div>
      </div>
    </div>
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
  </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Teddy-XiongGZ/MIRAGE" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
